{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPW9vm5kjzW5MdiC9N1WHVe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yqbg39rt108n","executionInfo":{"status":"ok","timestamp":1733939422299,"user_tz":-330,"elapsed":27787,"user":{"displayName":"Yuvraj G","userId":"06843858227456057452"}},"outputId":"5178d9c4-c25a-4a5b-b9bb-c1e98e0359a7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80NTpQ-V1TYx","executionInfo":{"status":"ok","timestamp":1733944472172,"user_tz":-330,"elapsed":1718698,"user":{"displayName":"Yuvraj G","userId":"06843858227456057452"}},"outputId":"d1c0f262-b32f-4d3b-b259-6e4b23e99207"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training and testing data loaded successfully.\n","Data preprocessing completed.\n","Random Forest model trained successfully.\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.66      0.21      0.32      3055\n","           1       0.68      0.94      0.79      5400\n","\n","    accuracy                           0.68      8455\n","   macro avg       0.67      0.58      0.55      8455\n","weighted avg       0.67      0.68      0.62      8455\n","\n","Confusion Matrix:\n"," [[ 655 2400]\n"," [ 344 5056]]\n","Training and testing data loaded successfully.\n","Data preprocessing completed.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [18:47:37] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost model trained successfully.\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.68      0.17      0.27      3055\n","           1       0.67      0.95      0.79      5400\n","\n","    accuracy                           0.67      8455\n","   macro avg       0.67      0.56      0.53      8455\n","weighted avg       0.67      0.67      0.60      8455\n","\n","Confusion Matrix:\n"," [[ 507 2548]\n"," [ 244 5156]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [19:14:25] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost model hyperparameter tuning completed. Best parameters:\n"," {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.9}\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00      3055\n","           1       0.64      1.00      0.78      5400\n","\n","    accuracy                           0.64      8455\n","   macro avg       0.32      0.50      0.39      8455\n","weighted avg       0.41      0.64      0.50      8455\n","\n","Confusion Matrix:\n"," [[   0 3055]\n"," [   0 5400]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","class BaseModel:\n","    def __init__(self):\n","        self.model = None\n","        self.scaler = StandardScaler()\n","        self.label_encoders = {}\n","\n","    def load(self, train_filepath, test_filepath):\n","        self.train_data = pd.read_excel(train_filepath)\n","        self.test_data = pd.read_excel(test_filepath)\n","        print(\"Training and testing data loaded successfully.\")\n","\n","    def preprocess(self):\n","        def process_data(data):\n","            # Feature engineering for transaction_date\n","            data['transaction_date'] = pd.to_datetime(data['transaction_date'])\n","            data['transaction_year'] = data['transaction_date'].dt.year\n","            data['transaction_month'] = data['transaction_date'].dt.month\n","\n","            # Drop unnecessary columns\n","            data = data.drop(['customer_id', 'transaction_date'], axis=1)\n","\n","            # Encode categorical variables\n","            categorical_cols = ['sub_grade', 'term', 'home_ownership', 'purpose', 'application_type', 'verification_status']\n","            for col in categorical_cols:\n","                if col not in self.label_encoders:\n","                    le = LabelEncoder()\n","                    data[col] = le.fit_transform(data[col])\n","                    self.label_encoders[col] = le\n","                else:\n","                    data[col] = self.label_encoders[col].transform(data[col])\n","\n","            # Scale numerical features\n","            numerical_cols = ['cibil_score', 'total_no_of_acc', 'annual_inc', 'int_rate',\n","                              'loan_amnt', 'installment', 'account_bal', 'emp_length', 'transaction_year', 'transaction_month']\n","            data[numerical_cols] = self.scaler.fit_transform(data[numerical_cols])\n","\n","            return data\n","\n","        self.train_data = process_data(self.train_data)\n","        self.test_data = process_data(self.test_data)\n","        print(\"Data preprocessing completed.\")\n","\n","    def split_data(self):\n","        X_train = self.train_data.drop('loan_status', axis=1)\n","        y_train = self.train_data['loan_status']\n","        X_test = self.test_data.drop('loan_status', axis=1)\n","        y_test = self.test_data['loan_status']\n","        return X_train, X_test, y_train, y_test\n","\n","    def train(self, X_train, y_train):\n","        raise NotImplementedError(\"Train method must be implemented by subclasses.\")\n","\n","    def test(self, X_test, y_test):\n","        y_pred = self.model.predict(X_test)\n","        report = classification_report(y_test, y_pred)\n","        cm = confusion_matrix(y_test, y_pred)\n","        print(\"Classification Report:\\n\", report)\n","        print(\"Confusion Matrix:\\n\", cm)\n","\n","    def predict(self, X):\n","        return self.model.predict(X)\n","\n","class RandomForestModel(BaseModel):\n","    def __init__(self):\n","        super().__init__()\n","        self.model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n","\n","    def train(self, X_train, y_train):\n","        self.model.fit(X_train, y_train)\n","        print(\"Random Forest model trained successfully.\")\n","\n","class XGBoostModel(BaseModel):\n","    def __init__(self):\n","        super().__init__()\n","        self.model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","\n","    def train(self, X_train, y_train):\n","        self.model.fit(X_train, y_train)\n","        print(\"XGBoost model trained successfully.\")\n","\n","    def tune_hyperparameters(self, X_train, y_train):\n","        xgb_param_grid = {\n","            'learning_rate': [0.01, 0.05, 0.1],\n","            'n_estimators': [50, 100, 200],\n","            'max_depth': [3, 5, 7],\n","            'subsample': [0.8, 0.9, 1.0],\n","            'colsample_bytree': [0.7, 0.8, 1.0]\n","        }\n","        grid_search = GridSearchCV(estimator=self.model, param_grid=xgb_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n","        grid_search.fit(X_train, y_train)\n","        self.model = grid_search.best_estimator_\n","        print(\"XGBoost model hyperparameter tuning completed. Best parameters:\\n\", grid_search.best_params_)\n","\n","# Example pipeline usage\n","if __name__ == \"__main__\":\n","    train_filepath = \"/content/train_data.xlsx\"\n","    test_filepath = \"/content/test_data.xlsx\"\n","\n","    # Random Forest pipeline\n","    rf_model = RandomForestModel()\n","    rf_model.load(train_filepath, test_filepath)\n","    rf_model.preprocess()\n","    X_train, X_test, y_train, y_test = rf_model.split_data()\n","    rf_model.train(X_train, y_train)\n","    rf_model.test(X_test, y_test)\n","\n","    # XGBoost pipeline\n","    xgb_model = XGBoostModel()\n","    xgb_model.load(train_filepath, test_filepath)\n","    xgb_model.preprocess()\n","    X_train, X_test, y_train, y_test = xgb_model.split_data()\n","    xgb_model.train(X_train, y_train)\n","    xgb_model.test(X_test, y_test)\n","\n","    # Hyperparameter tuning for XGBoost\n","    xgb_model.tune_hyperparameters(X_train, y_train)\n","    xgb_model.test(X_test, y_test)\n"]},{"cell_type":"markdown","source":["### Comparison of **Random Forest** and **XGBoost**\n","\n","\n","\n","---\n","\n","### **Random Forest**:\n","- **Accuracy**: 0.68\n","- **Recall (Class 1)**: 0.94\n","\n","**Explanation**: Random Forest has a **high recall of 0.94**, meaning it identifies 94% of defaulters correctly, but still misses 6% (false negatives). This makes it a solid choice when prioritizing minimizing false negatives.\n","\n","---\n","\n","### **XGBoost (Before Hyperparameter Tuning)**:\n","- **Accuracy**: 0.67\n","- **Recall (Class 1)**: 0.95\n","\n","**Explanation**: XGBoost outperforms Random Forest slightly in recall (**0.95**), meaning it identifies 95% of defaulters, missing only 5%. This is even better at minimizing false negatives compared to Random Forest.\n","\n","---\n","\n","### Conclusion:\n","\n","1. **XGBoost** is the better model for identifying defaulters, as it has a higher **recall (0.95)** compared to **Random Forest (0.94)**. This means XGBoost is better at minimizing false negatives (missed defaulters), which is crucial when ensuring that no defaulters are granted loans.\n","\n","2. **Hyperparameter tuning** for XGBoost has the potential to improve its performance. While the current tuned model achieves **perfect recall (1.00)**, the **precision drops**. Further fine-tuning of hyperparameters could help strike a better balance between recall and precision, ensuring optimal results for loan default prediction."],"metadata":{"id":"IeRcC7BySA6e"}}]}